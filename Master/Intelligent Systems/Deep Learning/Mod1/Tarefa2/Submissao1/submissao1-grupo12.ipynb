{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from ../models/dnn.pkl\n",
      "Previsões salvas em results/submissao1-grupo011-s1.csv com IDs\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../DNN')\n",
    "from neuralnet import NeuralNetwork, Dataset\n",
    "from optimizer import Optimizer\n",
    "from losses import BinaryCrossEntropy\n",
    "from metrics import accuracy\n",
    "from activation import ReLUActivation, SigmoidActivation\n",
    "from layers import DenseLayer, DropoutLayer\n",
    "import pickle\n",
    "\n",
    "# Caminhos dos arquivos (na mesma pasta)\n",
    "MODEL_PATH = \"../models/dnn.pkl\"\n",
    "INPUT_FILE = \"dataset2_inputs.csv\"\n",
    "OUTPUT_FILE = \"results/submissao1-grupo011-s1.csv\"\n",
    "\n",
    "# Ler os novos dados de entrada, extraindo IDs e textos\n",
    "ids = []\n",
    "texts = []\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    header = lines[0].strip()\n",
    "    if 'ID' not in header or 'Text' not in header:\n",
    "        print(f\"Erro: Cabeçalho deve conter 'ID' e 'Text' em {INPUT_FILE}\")\n",
    "        exit(1)\n",
    "    for line in lines[1:]:\n",
    "        parts = line.split(';', 1)  # Dividir no primeiro ';'\n",
    "        if len(parts) > 1:\n",
    "            id_part = parts[0].strip()  # ID é a parte antes do primeiro ';'\n",
    "            text = parts[1].strip()     # Texto é tudo após o primeiro ';'\n",
    "            ids.append(id_part)\n",
    "            texts.append(text)\n",
    "        else:\n",
    "            ids.append('')\n",
    "            texts.append('')\n",
    "\n",
    "# Criar o modelo básico sem adicionar camadas ainda\n",
    "net = NeuralNetwork(\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.005,\n",
    "    optimizer=Optimizer(learning_rate=0.005, momentum=0.9, weight_decay=1e-5),\n",
    "    loss=BinaryCrossEntropy,\n",
    "    metric=accuracy\n",
    ")\n",
    "\n",
    "# Carregar o modelo e o vectorizer\n",
    "try:\n",
    "    vectorizer = net.load(MODEL_PATH)\n",
    "    if vectorizer is None:\n",
    "        print(\"Erro: Vectorizer não encontrado no arquivo do modelo.\")\n",
    "        exit(1)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo {MODEL_PATH} não encontrado.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o modelo: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Transformar os novos dados com o vectorizer carregado\n",
    "X = vectorizer.transform(texts).toarray()  # Apenas transformar, usando o vectorizer do treino\n",
    "\n",
    "# Adicionar as camadas agora que sabemos o input_shape\n",
    "n_features = X.shape[1]\n",
    "net.add(DenseLayer(32, (n_features,)))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.3))\n",
    "net.add(DenseLayer(16))\n",
    "net.add(ReLUActivation())\n",
    "net.add(DropoutLayer(0.4))\n",
    "net.add(DenseLayer(1))\n",
    "net.add(SigmoidActivation())\n",
    "\n",
    "# Fazer previsões\n",
    "input_dataset = Dataset(X, None)\n",
    "predictions = net.predict(input_dataset)\n",
    "predictions_classes = np.round(predictions).astype(int)\n",
    "predictions_labels = ['IA' if pred == 1 else 'Human' for pred in predictions_classes.flatten()]\n",
    "\n",
    "# Criar DataFrame com IDs e previsões\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'Label': predictions_labels\n",
    "})\n",
    "output_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Previsões salvas em {OUTPUT_FILE} com IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from ../models/rnn.pkl\n",
      "Previsões salvas em results/submissao1-grupo011-s2.csv com IDs\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../RNN')\n",
    "from rnn import RecurrentNeuralNetwork  # Import the custom RNN model\n",
    "from optimizer import Optimizer  # Optimizer used for training\n",
    "from losses import BinaryCrossEntropy  # Loss function\n",
    "from metrics import accuracy  # Metric for evaluation\n",
    "from activation import ReLUActivation, SigmoidActivation  # Activation functions\n",
    "from layers import RecurrentLayer, DropoutLayer, DenseLayer  # RNN layers\n",
    "import pickle\n",
    "\n",
    "# Caminhos dos arquivos (na mesma pasta)\n",
    "MODEL_PATH = \"../models/rnn.pkl\"  # Updated model path for RNN\n",
    "INPUT_FILE = \"dataset2_inputs.csv\"  # Input dataset\n",
    "OUTPUT_FILE = \"results/submissao1-grupo011-s2.csv\"  # Output predictions\n",
    "\n",
    "# Ler os novos dados de entrada, extraindo IDs e textos\n",
    "ids = []\n",
    "texts = []\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    header = lines[0].strip()\n",
    "    if 'ID' not in header or 'Text' not in header:\n",
    "        print(f\"Erro: Cabeçalho deve conter 'ID' e 'Text' em {INPUT_FILE}\")\n",
    "        exit(1)\n",
    "    for line in lines[1:]:\n",
    "        parts = line.split(';', 1)  # Dividir no primeiro ';'\n",
    "        if len(parts) > 1:\n",
    "            id_part = parts[0].strip()  # ID é a parte antes do primeiro ';'\n",
    "            text = parts[1].strip()     # Texto é tudo após o primeiro ';'\n",
    "            ids.append(id_part)\n",
    "            texts.append(text)\n",
    "        else:\n",
    "            ids.append('')\n",
    "            texts.append('')\n",
    "\n",
    "# Carregar o modelo e o vectorizer\n",
    "try:\n",
    "    rnn = RecurrentNeuralNetwork(\n",
    "        epochs=10,\n",
    "        batch_size=16,\n",
    "        learning_rate=0.005,\n",
    "        optimizer=Optimizer(learning_rate=0.005, momentum=0.9, weight_decay=1e-5),\n",
    "        loss=BinaryCrossEntropy,\n",
    "        metric=accuracy\n",
    "    )\n",
    "    \n",
    "    vectorizer = rnn.load(MODEL_PATH)\n",
    "    if vectorizer is None:\n",
    "        print(\"Erro: Vectorizer não encontrado no arquivo do modelo.\")\n",
    "        exit(1)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo {MODEL_PATH} não encontrado.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o modelo: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Transformar os novos dados com o vectorizer carregado\n",
    "X = vectorizer.transform(texts).toarray()\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))  # Redimensionar para sequência (samples, time_steps, features)\n",
    "\n",
    "# Adicionar camadas RNN\n",
    "n_features = X.shape[2]\n",
    "rnn.add(RecurrentLayer(32, (n_features,)))  # Camada recorrente\n",
    "rnn.add(DropoutLayer(0.3))  # Dropout layer\n",
    "rnn.add(DenseLayer(16))  # Camada densa após a camada RNN\n",
    "rnn.add(ReLUActivation())  # ReLU Activation\n",
    "rnn.add(DropoutLayer(0.4))\n",
    "rnn.add(DenseLayer(1))  # Saída final\n",
    "rnn.add(SigmoidActivation())  # Sigmoid activation para classificação binária\n",
    "\n",
    "# Fazer previsões\n",
    "input_dataset = Dataset(X, None)\n",
    "predictions = rnn.predict(input_dataset)\n",
    "predictions_classes = np.round(predictions).astype(int)\n",
    "predictions_labels = ['IA' if pred == 1 else 'Human' for pred in predictions_classes.flatten()]\n",
    "\n",
    "# Criar DataFrame com IDs e previsões\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'Label': predictions_labels\n",
    "})\n",
    "\n",
    "# Salvar as previsões em um arquivo CSV\n",
    "output_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Previsões salvas em {OUTPUT_FILE} com IDs\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envDAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
